Thu Jul 4 19:15:35 EDT 2024: Job 12851469 is allocated resources.
Inside slurm_launcher.slrm (/var/spool/slurmd/job12851469/slurm_script). received arguments: main.py --output-dir results/models/laplace --model-name equiformer_v2 --data-path datasets/qm9 --model-type Laplace --epochs 2 --iterations 30 --seed 0
Namespace(output_dir='results/models/laplace', model_name='equiformer_v2', model_type='Laplace', input_irreps=None, radius=5.0, num_basis=32, output_channels=1, lmax=4, epochs=2, iterations=30, batch_size=64, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path_rate=0.1, patience=50, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.01, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=100, target=4, data_path='datasets/qm9', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=4, pin_mem=True, amp=False, prototyping=True, noise_sigma=0.02, denoising_coef=0.1, denoising_prob=0.5, corrupt_ratio=0.125)
500 response executing GraphQL.
{"error":"An internal error occurred. Please contact support."}
wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: Network error (HTTPError), entering retry loop.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /fs01/home/aguzmanc/3D_Bayes/wandb/run-20240704_191546-4s6mcvpm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Laplace, 0, L=M=4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/andresguzco/3D_Bayes
wandb: üöÄ View run at https://wandb.ai/andresguzco/3D_Bayes/runs/4s6mcvpm
wandb: 500 encountered ({"error":"An internal error occurred. Please contact support."}), retrying request
wandb: Network error resolved after 0:01:29.193763, resuming normal operation.
Training set mean: 6.820860385894775, std:1.4590328931808472
EquiformerV2(
  (sphere_embedding): Embedding(90, 128)
  (distance_expansion): GaussianSmearing()
  (SO3_rotation): ModuleList(
    (0): SO3_Rotation(
      (mapping): CoefficientMappingModule(lmax_list=[4], mmax_list=[4])
    )
  )
  (mappingReduced): CoefficientMappingModule(lmax_list=[4], mmax_list=[4])
  (SO3_grid): (4, 4)
  (edge_degree_embedding): EdgeDegreeEmbedding(
    (SO3_rotation): ModuleList(
      (0): SO3_Rotation(
        (mapping): CoefficientMappingModule(lmax_list=[4], mmax_list=[4])
      )
    )
    (mappingReduced): CoefficientMappingModule(lmax_list=[4], mmax_list=[4])
    (source_embedding): Embedding(90, 64)
    (target_embedding): Embedding(90, 64)
    (rad_func): RadialFunction(
      (net): Sequential(
        (0): Linear(in_features=728, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=640, bias=True)
      )
    )
  )
  (blocks): ModuleList(
    (0-1): 2 x TransBlockV2(
      (norm_1): EquivariantRMSNormArraySphericalHarmonicsV2(lmax=4, num_channels=128, eps=1e-05, centering=True, std_balance_degrees=True)
      (ga): SO2EquivariantGraphAttention(
        (SO3_rotation): ModuleList(
          (0): SO3_Rotation(
            (mapping): CoefficientMappingModule(lmax_list=[4], mmax_list=[4])
          )
        )
        (mappingReduced): CoefficientMappingModule(lmax_list=[4], mmax_list=[4])
        (SO3_grid): (4, 4)
        (source_embedding): Embedding(90, 64)
        (target_embedding): Embedding(90, 64)
        (so2_conv_1): SO2_Convolution(
          (mappingReduced): CoefficientMappingModule(lmax_list=[4], mmax_list=[4])
          (fc_m0): Linear(in_features=1280, out_features=896, bias=True)
          (so2_m_conv): ModuleList(
            (0): SO2_m_Convolution(
              (fc): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (1): SO2_m_Convolution(
              (fc): Linear(in_features=768, out_features=768, bias=False)
            )
            (2): SO2_m_Convolution(
              (fc): Linear(in_features=512, out_features=512, bias=False)
            )
            (3): SO2_m_Convolution(
              (fc): Linear(in_features=256, out_features=256, bias=False)
            )
          )
          (rad_func): RadialFunction(
            (net): Sequential(
              (0): Linear(in_features=728, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=3840, bias=True)
            )
          )
        )
        (alpha_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (alpha_act): SmoothLeakyReLU(negative_slope=0.2)
        (alpha_dropout): Dropout(p=0.1, inplace=False)
        (s2_act): SeparableS2Activation(
          (scalar_act): SiLU()
          (s2_act): S2Activation(
            (act): SiLU()
          )
        )
        (so2_conv_2): SO2_Convolution(
          (mappingReduced): CoefficientMappingModule(lmax_list=[4], mmax_list=[4])
          (fc_m0): Linear(in_features=640, out_features=320, bias=True)
          (so2_m_conv): ModuleList(
            (0): SO2_m_Convolution(
              (fc): Linear(in_features=512, out_features=512, bias=False)
            )
            (1): SO2_m_Convolution(
              (fc): Linear(in_features=384, out_features=384, bias=False)
            )
            (2): SO2_m_Convolution(
              (fc): Linear(in_features=256, out_features=256, bias=False)
            )
            (3): SO2_m_Convolution(
              (fc): Linear(in_features=128, out_features=128, bias=False)
            )
          )
        )
        (proj): SO3_LinearV2(in_features=64, out_features=128, lmax=4)
      )
      (drop_path): GraphDropPath(drop_prob=0.05)
      (norm_2): EquivariantRMSNormArraySphericalHarmonicsV2(lmax=4, num_channels=128, eps=1e-05, centering=True, std_balance_degrees=True)
      (ffn): FeedForwardNetwork(
        (SO3_grid): (4, 4)
        (so3_linear_1): SO3_LinearV2(in_features=128, out_features=415, lmax=4)
        (gating_linear): Linear(in_features=128, out_features=415, bias=True)
        (s2_act): SeparableS2Activation(
          (scalar_act): SiLU()
          (s2_act): S2Activation(
            (act): SiLU()
          )
        )
        (so3_linear_2): SO3_LinearV2(in_features=415, out_features=128, lmax=4)
      )
    )
  )
  (norm): EquivariantRMSNormArraySphericalHarmonicsV2(lmax=4, num_channels=128, eps=1e-05, centering=True, std_balance_degrees=True)
  (energy_block): FeedForwardNetwork(
    (SO3_grid): (4, 4)
    (so3_linear_1): SO3_LinearV2(in_features=128, out_features=415, lmax=4)
    (gating_linear): Linear(in_features=128, out_features=415, bias=True)
    (s2_act): SeparableS2Activation(
      (scalar_act): SiLU()
      (s2_act): S2Activation(
        (act): SiLU()
      )
    )
    (so3_linear_2): SO3_LinearV2(in_features=415, out_features=1, lmax=4)
  )
)
Number of params: 9939670
Initiating training.

Starting validation step for early stopping.

Epoch: [0][0/8] 	 loss: 1.13104, MAE: 1.65023, time/step=757ms, lr=1.00e-06
Epoch: [0][7/8] 	 loss: 0.82895, MAE: 1.20947, time/step=386ms, lr=1.00e-06
Epoch: [0] Target: [4] train MAE: 1.20947, val MAE: 1.07218, Time: 3.81s
Epoch: [1][0/8] 	 loss: 0.84046, MAE: 1.22626, time/step=108ms, lr=1.01e-04
Epoch: [1][7/8] 	 loss: 0.71471, MAE: 1.04279, time/step=303ms, lr=1.01e-04
Epoch: [1] Target: [4] train MAE: 1.04279, val MAE: 0.87041, Time: 3.16s
Best -- epoch=1, train MAE: 1.04279, val MAE: 0.87041.

Early stopping computation complete.

Epoch: [0][0/10] 	 loss: 0.47515, MAE: 0.69326, time/step=101ms, lr=1.00e-06
Epoch: [0][9/10] 	 loss: 0.91175, MAE: 1.33027, time/step=305ms, lr=1.00e-06
Epoch: [0] Target: [4] train MAE: 1.33027, Time: 3.27s
Epoch: [1][0/10] 	 loss: 0.74634, MAE: 1.08893, time/step=98ms, lr=1.01e-04
Epoch: [1][9/10] 	 loss: 0.89017, MAE: 1.29878, time/step=295ms, lr=1.01e-04
Epoch: [1] Target: [4] train MAE: 1.29878, Time: 3.17s
Best -- epoch=1, train MAE: 1.29878.

Laplace Layer Parameters: 2076
Laplace fitted. Time taken: 47.72
Prior precision optimised. Time: 49.88

Evaluating model.
slurmstepd: error: *** JOB 12851469 ON gpu139 CANCELLED AT 2024-07-04T19:19:06 ***
